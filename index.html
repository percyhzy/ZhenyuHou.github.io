<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Yunlong Song</title>
    <link rel="stylesheet" href="style.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Font Awesome for modern icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <!-- Google Fonts for academic typography -->
    <link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@300;400;700&family=Lora:wght@400;500;600&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <div class="sidebar">
            <div class="profile-image-container">
                <img src="images/me.jpg" alt="Yunlong Song" class="profile-image">
            </div>
            <div class="sidebar-links">
                <a href="https://scholar.google.com/citations?user=EzAXL9QAAAAJ&hl=en" title="Google Scholar">
                    <i class="fas fa-graduation-cap"></i>
                </a>
                <a href="https://x.com/realyunlong" title="Twitter">
                    <i class="fab fa-x-twitter"></i>
                </a>
                <a href="https://github.com/yun-long" title="GitHub">
                    <i class="fab fa-github"></i>
                </a>
            </div>
        </div>
        <div class="main-content">
            <h1>Yunlong Song - 宋运龙</h1>

            <section class="intro">
              <p>
                I am currently a Research Scientist at Skild AI, developing robot learning algorithms for general-purpose robot manipulation.
                I completed my PhD under the supervision of <a href="https://rpg.ifi.uzh.ch/people_scaramuzza.html">Prof. Davide Scaramuzza</a> at
                the <a href="https://rpg.ifi.uzh.ch/">Robotics and Perception Group</a>, which is part of the Department
                of Neuroinformatics, a joint institute of <a href="https://www.uzh.ch/en.html">University of Zurich</a>
                and <a href="https://ethz.ch/en.html">ETH Zurich</a>. During my PhD, I also conducted research
                at <a href="https://www.mit.edu/">MIT</a>, working with
                <a href="https://meche.mit.edu/people/faculty/SANGBAE@MIT.EDU">Prof. Sangbae Kim</a> on quadruped locomotion.
                Additionally, I had the opportunity to collaborate with <a href="https://www.gran-turismo.com/us/gran-turismo-sophy/project/">SONY AI</a>
                on building super-human game agent for Gran Turismo and with <a href="https://vladlen.info/">Vladlen Koltun</a> on autonomous drone racing.
                Prior to my PhD, I earned my master's degree from <a href="https://www.tu-darmstadt.de/index.en.jsp">TU Darmstadt</a>,
                where I was supervised by <a href="https://www.ias.informatik.tu-darmstadt.de/Member/JanPeters">Prof. Jan Peters</a>.
              </p>

              <p>
                I was born and grew up in <span class="platform-image" data-image="images/home.jpg"> small village</span>
                in South China, where access to quality education was extremely limited due to poverty. From the age of six,
                I worked on farms, gaining over a decade of hands-on experience in agriculture. I later worked in various
                Chinese factories and financed my Master’s studies in Germany through part-time jobs at a software company
                and as a student assistant at the university.
                Having personally experienced the challenges of educational barriers, I am deeply committed to breaking
                them down—whether by directly contributing to education accessibility or helping others pursue better
                learning opportunities.
              </p>

              <a href="https://youtu.be/DskNmbKzwf0?si=PYfEU-9RyGKbAZt2">PhD Defense</a>


            </section>

            <hr>

            <section class="research-interests">
                <h2>Research Interests</h2>
                <p>
                    My research is centered on developing intelligent robotic systems that can learn, adapt, and interact
                    in complex real-world environments. I have worked on a variety of topics in robotics and machine learning, including
                    optimal control, reinforcement learning, differentiable simulation, representation learning, and
                    vision-based control. Additionally, I have hands-on experience with a variety of robotic platforms,
                    including <span class="platform-image" data-image="images/furuta.jpg">Furuta Pendulum</span>,
                    <span class="platform-image" data-image="images/wam_da.jpg">Barrett WAM Arm</span>,
                    <span class="platform-image" data-image="images/drone.jpg">high-performance FPV drone</span>,
                    <span class="platform-image" data-image="images/mit_cheetah.jpg">MIT Mini Cheetah</span>, and manipulators.
                    My past research has been mainly focused on developing robust skill-level policies for different robotic tasks.
                    Currently, I am interested in combining the power large vision-language-action models with low-level
                    skills to enable robots to perform complex tasks in the real world.
                    My current research interests include:
                </p>
                <ul>
                    <li>Real-world Reinforcement Learning</li>
                    <li>Reinforcement Learning for Mobile/Whole-body Manipulation</li>
                    <li>Direct Policy Optimization for Pixel-to-Action Control</li>
                    <li>Post-training of Robotic Foundation Models</li>
                    <li>Multimodal Policy Learning</li>
                </ul>
            </section>

            <hr>

            <section class="publications">
                <h2>Selected Pulications</h2>
                Some peronal favorites from my publication list:
                <ul>
                    <li>
                        <div class="publication-details">
                          <strong>Learning Vision-based Agile Flight via Differentiable Physics</strong>
                          <div class="publication-authors">
                            Y. Zhang*, Y. Hu*, Y. Song*, D. Zou and W. Lin
                          </div>
                          <div class="publication-venue-links">
                            <span class="publication-venue">arXiv, 2024</span>
                            <span class="paper-links">
                                <a href="https://arxiv.org/abs/2407.10648">PDF</a>
                                <a href="https://youtu.be/LKg9hJqc2cc">Video</a>
                            </span>
                          </div>
                        </div>
                    </li>

                    <li>
                        <div class="publication-details">
                          <strong>Learning Quadruped Locomotion Using Differentiable Simulation</strong>
                          <div class="publication-authors">
                              Y. Song, S. Kim, and D. Scaramuzza
                          </div>
                          <div class="publication-venue-links">
                            <span class="publication-venue">CoRL, 2024</span>
                            <span class="paper-links">
                                <a href="https://openreview.net/forum?id=XopATjibyz">PDF</a>
                                <a href="https://youtu.be/weNq_w715xM">Video</a>
                            </span>
                          </div>
                        </div>
                    </li>
                    <!--  -->
                    <li>

                        <div class="publication-details">
                            <strong>Reaching the Limit in Autonomous Racing: Optimal Control Versus Reinforcement Learning</strong>
                            <div class="publication-authors">
                                Y. Song, A. Romero, M. Müller, V. Koltun, and D. Scaramuzza
                            </div>
                            <div class="publication-venue-links">
                                <span class="publication-venue">Science Robotics, 2023 (featured on the Cover)</span>
                                <span class="paper-links">
                                    <a href="https://www.science.org/stoken/author-tokens/ST-1485/full">PDF</a>
                                    <a href="https://youtu.be/HGULBBAo5lA">Video</a>
                                </span>
                            </div>
                        </div>
                    </li>
                    <!--  -->
                    <li>

                        <div class="publication-details">
                            <strong>Policy Search for Model Predictive Control for Agile Drone Flight</strong>
                            <div class="publication-authors">
                                Y. Song and D. Scaramuzza
                            </div>
                            <div class="publication-venue-links">
                              <span class="publication-venue">IEEE T-RO, 2022</span>
                              <span class="paper-links">
                                  <a href="https://ieeexplore.ieee.org/abstract/document/9719129">PDF</a>
                                  <a href="https://youtu.be/Qei7oGiEIxY">Video</a>
                              </span>
                            </div>
                        </div>
                    </li>
                    <!--  -->
                    <li>

                        <div class="publication-details">
                            <strong>Flightmare: A Flexible Quadrotor Simulator</strong>
                            <div class="publication-authors">
                                Y. Song, S. Naji, E. Kaufmann, A. Loquercio, D. Scaramuzza
                            </div>
                            <div class="publication-venue-links">
                            <span class="publication-venue">CoRL, 2021</span>
                              <span class="paper-links">
                                  <a href="https://proceedings.mlr.press/v155/song21a.html">PDF</a>
                                  <a href="https://youtu.be/m9Mx1BCNGFU">Video</a>
                              </span>
                            </div>
                        </div>
                    </li>
                    <!--  -->
                    <li>

                        <div class="publication-details">
                            <strong>Learning High-level Policies for Model Predictive Control</strong>
                            <div class="publication-authors">
                                Y. Song and D. Scaramuzza
                            </div>
                            <div class="publication-venue-links">
                              <span class="publication-venue">IRSO, 2020 (my very first publication)</span>
                              <span class="paper-links">
                                  <a href="https://arxiv.org/abs/2007.10284">PDF</a>
                                  <a href="https://youtu.be/2uQcRnp7yI0">Video</a>
                              </span>
                            </div>
                        </div>
                    </li>
                </ul>
            </section>

            <hr>
              <section class="project">
                <h2>Research Projects</h2>
                <p>
                  I am passionate about developing algorithms for robots that can learn to perform complex tasks in the real world.
                  Here are some of the projects I have worked on:
                </p>
                <div class="project-list">

                    <div class="project-item">
                        <div class="project-header">
                            <a href="#" class="project-title" onclick="toggleDescription(this)">
                                Pixel-to-Action Control
                                <span class="toggle-icon">▼</span>
                            </a>
                        </div>
                        <div class="project-description">
                            <div class="video-text-container">
                                <p>
                                  Learning pixel-to-action control policies is challenging due to the high-dimensional
                                  and partially observable nature of the problem. This work
                                  combines deep learning with first-principle
                                  physics through differentiable simulation to
                                  enable autonomous navigation of multiple
                                  aerial robots through complex environments at
                                  high speed. Our approach optimizes a neural
                                  network control policy directly by
                                  backpropagating loss gradients through the
                                  robot simulation using a simple point-mass
                                  physics model and a depth rendering engine.
                                  Despite this simplicity, our method excels in
                                  challenging tasks for both multi-agent and
                                  single-agent applications with zero-shot
                                  sim-to-real transfer. In multi-agent
                                  scenarios, our system demonstrates
                                  self-organized behavior, enabling autonomous
                                  coordination without communication or
                                  centralized planning - an achievement not
                                  seen in existing traditional or
                                  learning-based methods. In real-world forest
                                  environments, it navigates at speeds up to 20
                                  m/s. All these capabilities are deployed on a
                                  budget-friendly $21 computer, costing less
                                  than 5% of a GPU-equipped board used in
                                  existing systems.

                                </p>
                                <video class="project-video project-video-small" autoplay loop muted playsinline>
                                    <source src="videos/vision_swarm.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                    </div>

                    <div class="project-item">
                        <div class="project-header">
                            <a href="#" class="project-title" onclick="toggleDescription(this)">
                                Quadruped Locomotion
                                <span class="toggle-icon">▼</span>
                            </a>
                        </div>
                        <div class="project-description">
                            <div class="video-text-container">
                                <p>
                                    This project presents one of the
                                    first successful applications of differentiable
                                    simulation for real-world quadruped locomotion,
                                    offering a compelling alternative to traditional
                                    RL methods. Differentiable simulation promises
                                    fast convergence and stable training by computing
                                    low-variance first-order gradients using robot
                                    dynamics. However, its usage for legged robots is
                                    still limited to simulation. The main challenge
                                    lies in the complex optimization landscape of
                                    robotic tasks due to discontinuous dynamics. This
                                    work proposes a new differentiable simulation
                                    framework to overcome these challenges.
                                </p>
                                <video class="project-video project-video-small" autoplay loop muted playsinline>
                                    <source src="videos/cheetah.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                    </div>


                    <div class="project-item">
                        <div class="project-header">
                            <a href="#" class="project-title" onclick="toggleDescription(this)">
                                Super-human Performance in Autonomous Racing
                                <span class="toggle-icon">▼</span>
                            </a>
                        </div>
                        <div class="project-description">
                            <!-- <img src="images/01_racing.gif" alt="Drone Racing" class="project-image"> -->
                            <div class="video-text-container">
                                <p>
                                  How can we push a robot to its absolut limits in the physical world?
                                  We developed one of the world fastest autonomous drone and pushed this machine to its maximum performance
                                  in the real world, achieving a peak acceleration greater than 12g and
                                  a peak velocity of 108 km/h.
                                  The key to our success is a neural network policy trained with reinforcement learning.
                                  Our neural network policy achieved superhuman control performance within minutes of training on
                                  a standard workstation. Additinally, our study indicates that the fundamental advantage of
                                  reinforcement learning over optimal control is not that
                                  it optimizes its objective better but that it optimizes a better objective.
                                  RL can directly optimize a task-level objective and can leverage domain randomization
                                  to cope with model uncertainty, allowing the discovery of more robust control responses.
                                </p>
                                <video class="project-video project-video-small" autoplay loop muted playsinline>
                                    <source src="videos/splits.mp4" type="video/mp4">
                                </video>
                            </div>
                            <!-- <div class="related-publication"> -->
                            <!--     <h3>Related Publication</h3> -->
                            <!--     <div class="publication-details"> -->
                            <!--         <strong><a href="https://ieeexplore.ieee.org/abstract/document/9719129">Policy Search for Model Predictive Control for Agile Drone Flight</a></strong> -->
                            <!--         <div class="publication-authors"> -->
                            <!--             Y. Song and D. Scaramuzza. IEEE T-RO, 2022 -->
                            <!--         </div> -->
                            <!--     </div> -->
                            <!-- </div> -->
                        </div>
                    </div>

                    <div class="project-item">
                        <div class="project-header">
                            <a href="#" class="project-title" onclick="toggleDescription(this)">
                                Optimal Control for Agile Flight
                                <span class="toggle-icon">▼</span>
                            </a>
                        </div>
                        <div class="project-description">
                            <!-- <img src="images/02_prompc.gif" alt="Pixel-to-Action Control" class="project-image"> -->
                            <div class="video-text-container">
                                <p>
                                  This project aims to design an optimal controller for agile drone flight. Model
                                  Predictive Control (MPC) provides near optimal performance by leveraging models and numerical
                                  optimization. However, a key challenge lies in defining an effective loss function with
                                  well-tuned hyperparameters, which is task-specific and difficult to search.
                                  To address this, we introduce a policy-search-for-model-predictive-control
                                  framework that employs policy search to automatically search high-level decision
                                  variables for MPC. Specifically, we formulate MPC as a parameterized controller, where
                                  traditionally hard-to-optimize decision variables are represented as high-level policies
                                  and learned through policy search. This approach enables MPC to adapt to changing environments.
                                </p>
                                <video class="project-video project-video-small" autoplay loop muted playsinline>
                                    <source src="videos/highmpc.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                    </div>

                    <div class="project-item">
                        <div class="project-header">
                            <a href="#" class="project-title" onclick="toggleDescription(this)">
                                Real-world Reinforcement Learning for Inverted Pendulum
                                <span class="toggle-icon">▼</span>
                            </a>
                        </div>
                        <div class="project-description">
                            <!-- <img src="images/02_prompc.gif" alt="Pixel-to-Action Control" class="project-image"> -->
                            <div class="video-text-container">
                                <p>
                                  Back to 2018, reinforcement learning for real-world robotic tasks was still in its infancy.
                                  During my master thesis, I developed a model-free reinforcement learning
                                  algorithm for the inverted pendulum task. The goal is to train a neural network policy to
                                  balance the Furuta pendulum in the real world. The policy is trained using Information-constrained
                                  Policy Optimization with a reward function that penalizes the distance between the pendulum and
                                  the upright position. After training the policy in simulation, we then fine-tuned it in the real world.
                                  The policy is able to balance the inverted pendulum in the real world, despite the presence of
                                  various disturbances. The Furuta-Pendulum is a rotational inverted pendulum, an
                                  under-actuated system invented by Katsuhisa Furuta and colleagues at Tokyo Institute
                                  of Technology in 1992. Since then, it has become a standard research platform for
                                  demonstrating performance of linear and non-linear control laws.
                                </p>
                                <video class="project-video project-video-small" autoplay loop muted playsinline>
                                    <source src="videos/pendulum.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                    </div>

                </div>
            </section>

            <hr>


            <section class="open-source">
                <h2>Open-Source Projects</h2>
                <div class="project-list">
                    <div class="project-item">
                        <div class="project-header">
                            <a href="https://github.com/uzh-rpg/flightmare" class="project-title" target="_blank">
                                Flightmare: A Flexible Quadrotor Simulator
                            </a>
                        </div>
                        <div class="project-description">
                            <p>
                                Flightmare is a flexible quadrotor simulator
                                that provides a high-fidelity rendering engine
                                and a CPU parallized physics engine for training reinforcement
                                learning control policies for quadrotor applications.
                            </p>
                            <div class="project-meta">
                                <span>C++</span> | <span>Python</span>| <span>Reinforcement Learning</span> | <span>Simulation</span>
                            </div>
                        </div>
                    </div>

                    <div class="project-item">
                        <div class="project-header">
                            <a href="https://github.com/uzh-rpg/agilicious?tab=readme-ov-file" class="project-title" target="_blank">
                               Agilicious: Open-source and Open-hardware Agile Quadrotor for Vision-based Flight
                            </a>
                        </div>
                        <div class="project-description">
                            <p>
                              Agilicious is a codesigned hardware and
                              software framework tailored to autonomous, agile
                              quadrotor flight. It  supports both model-based and
                              neural network–based controllers. Also, it provides
                              high thrust-to-weight and torque-to-inertia ratios
                              for agility, onboard vision sensors, graphics
                              processing unit (GPU)–accelerated compute hardware
                              for real-time perception and neural network
                              inference, a real-time flight controller, and a
                              versatile software stack.
                            </p>
                            <div class="project-meta">
                                <span>C++</span> | <span>Optimal Control</span> | <span>Robotics</span>
                            </div>
                        </div>
                    </div>

                    <div class="project-item">
                        <div class="project-header">
                            <a href="https://github.com/uzh-rpg/high_mpc" class="project-title" target="_blank">
                              Policy Search for Model Predictive Control
                            </a>
                        </div>
                        <div class="project-description">
                            <p>
                              A minimal implementation of policy search for model predictive control.
                            </p>
                            <div class="project-meta">
                                <span>Optimal Control</span> | <span>Python</span> | <span>Casadi</span>
                            </div>
                        </div>
                    </div>

                    <div class="project-item">
                        <div class="project-header">
                            <a href="https://github.com/yun-long/Foundations-of-Robotics" class="project-title" target="_blank">
                              Foundations of Robotics
                            </a>
                        </div>
                        <div class="project-description">
                            <p>
                              This repository provides a collection of Online
                              Lectures and Lecture Notes about Robotics. The design principle of
                              those notes is to be concise, clear, and
                              comprehensive, namely, less is more. Whether you're
                              a beginner or looking to deepen your understanding,
                              this resource aims to lay a solid foundation in
                              robotics.
                            </p>
                            <div class="project-meta">
                                <span>Optimal Control</span> | <span>Reinforcement Learning</span> | <span>Deep Learning</span> | <span>Computer Vision</span> | <span>LaTeX</span>
                            </div>
                        </div>
                    </div>

                    <!-- <div class="project-item"> -->
                    <!--     <div class="project-header"> -->
                    <!--         <a href="https://github.com/yun-long/yun-long.github.io" class="project-title" target="_blank"> -->
                    <!--             Personal Website -->
                    <!--         </a> -->
                    <!--     </div> -->
                    <!--     <div class="project-description"> -->
                    <!--         <p> -->
                    <!--             A minimalist, responsive personal website showcasing research, projects, -->
                    <!--             and professional journey, built with a focus on clean design and accessibility. -->
                    <!--         </p> -->
                    <!--         <div class="project-meta"> -->
                    <!--             <span>HTML</span> | <span>CSS</span> | <span>JavaScript</span> -->
                    <!--         </div> -->
                    <!--     </div> -->
                    <!-- </div> -->

                </div>
            </section>

        </div>

    <!-- Image Modal -->
    <div id="imageModal" class="modal">
        <span class="close-modal">&times;</span>
        <img class="modal-content" id="modalImage">
    </div>
    <script>

    function toggleDescription(element) {
                // Prevent default link behavior
                event.preventDefault();

                // Find the description div
                const description = element.closest('.project-item').querySelector('.project-description');
                const toggleIcon = element.querySelector('.toggle-icon');

                // Toggle visibility
                if (description.style.display === 'none') {
                    description.style.display = 'block';
                    toggleIcon.textContent = '▲';
                } else {
                    description.style.display = 'none';
                    toggleIcon.textContent = '▼';
                }
            }

            // On page load, set all project descriptions to be visible by default
            document.addEventListener('DOMContentLoaded', function() {
                const projectDescriptions = document.querySelectorAll('.project-description');
                const toggleIcons = document.querySelectorAll('.toggle-icon');
                const platformImages = document.querySelectorAll('.platform-image');
                const modal = document.getElementById('imageModal');
                const modalImage = document.getElementById('modalImage');
                const closeModal = document.querySelector('.close-modal');

                projectDescriptions.forEach(description => {
                    description.style.display = 'block';
                });

                toggleIcons.forEach(icon => {
                    icon.textContent = '▲';
                });

                // Image Modal Logic
                platformImages.forEach(platform => {
                    platform.addEventListener('click', function() {
                        const imageName = this.getAttribute('data-image');
                        modalImage.src = imageName;
                        modal.style.display = 'flex';
                    });
                });

                closeModal.onclick = function() {
                    modal.style.display = 'none';
                }

                window.onclick = function(event) {
                    if (event.target === modal) {
                        modal.style.display = 'none';
                    }
                }
            });
    </script>

</body>
</html>

